{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import keras\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import optimizers\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.6\n"
     ]
    }
   ],
   "source": [
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forcing keras to use the gpu\n",
    "config = tf.ConfigProto( device_count = {'GPU': 1 , 'CPU': 4} ) \n",
    "sess = tf.Session(config=config) \n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inizialization of the dataset\n",
    "data = pd.DataFrame([['tweet molto negativo', 0]], columns=['text', 'sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    39822\n",
      "0    31435\n",
      "Name: sent, dtype: int64\n",
      "8387\n",
      "1    21480\n",
      "0    21390\n",
      "Name: sent, dtype: int64\n",
      "0    10045\n",
      "1     9955\n",
      "Name: sent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#loading the data from the KaggleTranslated dataset\n",
    "data_kaggle = pd.read_csv('training_data/KaggleTranslated.csv', encoding='latin_1', index_col = 0)\n",
    "data_kaggle = data_kaggle.rename(index=str, columns={\"SentimentText\": \"text\"})\n",
    "\n",
    "print(data_kaggle['sent'].value_counts())\n",
    "\n",
    "diff = abs(data_kaggle['sent'].value_counts()[0] - data_kaggle['sent'].value_counts()[1])\n",
    "print (diff)\n",
    "\n",
    "data_kaggle_pos = data_kaggle[data_kaggle['sent'] == 1]\n",
    "\n",
    "drop_indices = np.random.choice(data_kaggle_pos.index, diff, replace=False)\n",
    "\n",
    "data_kaggle = data_kaggle.drop(drop_indices) #balancing the dataset |pos| = |neg|\n",
    "\n",
    "data_kaggle = data_kaggle.sample(frac=1).reset_index(drop=True) #shuffling the dataset\n",
    "\n",
    "data_kaggle_test = data_kaggle[data_kaggle.index < 20000] #test-train split\n",
    "data_kaggle_train = data_kaggle[data_kaggle.index >= 20000]\n",
    "    \n",
    "print(data_kaggle_train['sent'].value_counts())\n",
    "print(data_kaggle_test['sent'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    2472\n",
      "1.0    1601\n",
      "Name: sent, dtype: int64\n",
      "871\n",
      "0.0    1751\n",
      "1.0    1601\n",
      "Name: sent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#loading data from sentipol16, data from an italian political sentiment competion\n",
    "data_pol = pd.read_csv('training_data/training_set_sentipolc16.csv', encoding = \"latin_1\")\n",
    "\n",
    "#deleting the unused collumns \n",
    "del data_pol['idtwitter'] \n",
    "del data_pol['iro']\n",
    "del data_pol['top']\n",
    "del data_pol['subj']\n",
    "\n",
    "#dropping mixed sentiment tweets\n",
    "data_pol = data_pol.drop(data_pol[(data_pol['opos'] == 1) & (data_pol['oneg'] == 1)].index)\n",
    "data_pol = data_pol.drop(data_pol[(data_pol['lpos'] == 1) & (data_pol['lneg'] == 1)].index)\n",
    "\n",
    "data_pol.loc[data_pol['lpos'] == 1, 'sent'] = 1\n",
    "data_pol.loc[data_pol['lneg'] == 1, 'sent'] = 0\n",
    "\n",
    "data_pol.loc[data_pol['opos'] == 1, 'sent'] = 1\n",
    "data_pol.loc[data_pol['oneg'] == 1, 'sent'] = 0\n",
    "\n",
    "\n",
    "data_pol = data_pol[['text','sent']]\n",
    "\n",
    "print(data_pol['sent'].value_counts())\n",
    "\n",
    "diff = data_pol['sent'].value_counts()[0] - data_pol['sent'].value_counts()[1]\n",
    "print (diff)\n",
    "d_neg = data_pol[data_pol['sent'] == 0]\n",
    "\n",
    "drop_indices = np.random.choice(d_neg.index, diff-150, replace=False)\n",
    "\n",
    "data_pol = data_pol.drop(drop_indices)#balancing the dataset \n",
    "    \n",
    "print(data_pol['sent'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    266\n",
      "1.0    234\n",
      "Name: sent, dtype: int64\n",
      "0.0    1485\n",
      "1.0    1367\n",
      "Name: sent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_pol = data_pol.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data_null = data_pol[np.isnan(data_pol['sent'])]\n",
    "drop_indices = np.random.choice(data_null.index, data_null.shape[0], replace=False)\n",
    "\n",
    "data_pol = data_pol.drop(drop_indices)\n",
    "\n",
    "data_pol = data_pol.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#train-test split\n",
    "data_pol_test = data_pol[data_pol.index < 500] \n",
    "data_pol_train = data_pol[data_pol.index >= 500]\n",
    "\n",
    "\n",
    "print(data_pol_test['sent'].value_counts())\n",
    "print(data_pol_train['sent'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    24361\n",
      "1.0    24214\n",
      "Name: sent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Combining the dataset,  \n",
    "data_pol = data_pol.sample(frac=1).reset_index(drop=True)\n",
    "data = data.append(data_kaggle_train,ignore_index=True)\n",
    "data = data.append(data_pol_train,  ignore_index=True)\n",
    "data = data.append(data_pol_train,  ignore_index=True)\n",
    "\n",
    "print(data['sent'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    24361\n",
      "1.0    24214\n",
      "Name: sent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Removing sensible words such as the names of candidates and parties\n",
    "data['text'] = data['text'].apply(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]','',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub(':\\)','',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub(':\\(','',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('#','',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('Berlusconi','akfha',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('Renzi','lafh',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('Salvini','kjahfka',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('Grillo','lhalahfl',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('berlusconi','akfha',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('renzi','lafh',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('salvini','kjahfka',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('grillo','lhalahfl',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('Lega','sds',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('PD ','fsfsfsf',x)))\n",
    "data['text'] = data['text'].apply((lambda x: re.sub('FI ','dsgsgsgs',x)))\n",
    "\n",
    "print(data['sent'].value_counts())\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#initializing the tokenizer\n",
    "num_words = 25000\n",
    "maxlen = 30\n",
    "tokenizer = Tokenizer(num_words=num_words, split=' ') \n",
    "tokenizer.fit_on_texts(data['text'].values)\n",
    "\n",
    "#tokening the dataset\n",
    "tweets = tokenizer.texts_to_sequences(data['text'].values)\n",
    "tweets = pad_sequences(tweets, maxlen=maxlen)\n",
    "\n",
    "#saving the tokenizer\n",
    "tokenizer_file = open(\"model_files/tokenizer.pickle\", \"wb\")\n",
    "pickle.dump(tokenizer, tokenizer_file)\n",
    "tokenizer_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 30, 128)           3200000   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 30, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 30, 196)           254800    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 98)                115640    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 198       \n",
      "=================================================================\n",
      "Total params: 3,570,638\n",
      "Trainable params: 3,570,638\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#building the model\n",
    "emb_dim = 128 #128\n",
    "lstm_size = 196 #196\n",
    "lstm2_size = 98\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, emb_dim,input_length = tweets.shape[1]))\n",
    "model.add(Dropout(0.5, noise_shape=None, seed=None))\n",
    "model.add(LSTM(lstm_size, dropout=0.5, recurrent_dropout=0.5,return_sequences=True))\n",
    "model.add(LSTM(lstm2_size, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(Dense(2,activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer= 'adam' ,metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43717, 30) (43717, 2)\n",
      "(4858, 30) (4858, 2)\n"
     ]
    }
   ],
   "source": [
    "labels = pd.get_dummies(data['sent']).values\n",
    "tweets_train, tweets_test, labels_train, labels_test = train_test_split(tweets,labels, test_size = 0.1, random_state = 42)\n",
    "print(tweets_train.shape,labels_train.shape)\n",
    "print(tweets_test.shape,labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "13s - loss: 0.6375 - acc: 0.6245\n",
      "Epoch 2/7\n",
      "13s - loss: 0.5188 - acc: 0.7477\n",
      "Epoch 3/7\n",
      "13s - loss: 0.4567 - acc: 0.7887\n",
      "Epoch 4/7\n",
      "13s - loss: 0.4151 - acc: 0.8157\n",
      "Epoch 5/7\n",
      "13s - loss: 0.3825 - acc: 0.8335\n",
      "Epoch 6/7\n",
      "13s - loss: 0.3608 - acc: 0.8452\n",
      "Epoch 7/7\n",
      "13s - loss: 0.3376 - acc: 0.8566\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efcfc4cf438>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 512\n",
    "model.fit(tweets_train, labels_train, epochs = 7, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096/4358 [===========================>..] - ETA: 0sscore: 0.55\n",
      "acc: 0.75\n"
     ]
    }
   ],
   "source": [
    "validation_size = 500\n",
    "\n",
    "tweets_validate = tweets_test[0:validation_size]\n",
    "labels_validate = labels_test[0:validation_size]\n",
    "tweets_test = tweets_test[validation_size:]\n",
    "labels_test = labels_test[validation_size:]\n",
    "score,acc = model.evaluate(tweets_test, labels_test, verbose = 1, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s\n",
      "score: 0.59\n",
      "acc: 0.73\n"
     ]
    }
   ],
   "source": [
    "score,acc = model.evaluate(tweets_validate, labels_validate, verbose = 1, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_files/ItalianSentCls.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19968/20000 [============================>.] - ETA: 0sscore: 0.59\n",
      "acc: 0.73\n"
     ]
    }
   ],
   "source": [
    "tweets_kaggle = data_kaggle_test['text'].values\n",
    "tweets_kaggle = tokenizer.texts_to_sequences(data_kaggle_test['text'].values)\n",
    "tweets_kaggle = pad_sequences(tweets_kaggle, maxlen=maxlen)\n",
    "labels_kaggle = pd.get_dummies(data_kaggle_test['sent']).values\n",
    "score,acc = model.evaluate(tweets_kaggle, labels_kaggle, verbose = 1, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-239113e7f5da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpos_cnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_cnt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_correct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_kaggle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_kaggle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtweets_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0003\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1515\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         return self._predict_loop(f, ins,\n\u001b[0;32m-> 1517\u001b[0;31m                                   batch_size=batch_size, verbose=verbose)\n\u001b[0m\u001b[1;32m   1518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m                 \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
    "for x in range(len(tweets_kaggle)):    \n",
    "    result = model.predict(tweets_kaggle[x].reshape(1,tweets_test.shape[1]),batch_size=1,verbose = 2)[0]   \n",
    "    if random.uniform(0, 1) < 0.0003:\n",
    "        print(result)\n",
    "        print(x)   \n",
    "    if result[0]>result[1]:\n",
    "        conf = result[0]\n",
    "    else:\n",
    "        conf = result[1]   \n",
    "    if conf > 0.80:\n",
    "        if np.argmax(result) == np.argmax(labels_kaggle[x]):\n",
    "            if np.argmax(labels_kaggle[x]) == 0:\n",
    "                neg_correct += 1\n",
    "            else:\n",
    "                pos_correct += 1\n",
    "\n",
    "        if np.argmax(labels_kaggle[x]) == 0:\n",
    "            neg_cnt += 1\n",
    "        else:\n",
    "            pos_cnt += 1\n",
    "\n",
    "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\", pos_cnt)\n",
    "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\", neg_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 0s\n",
      "score: 0.61\n",
      "acc: 0.72\n"
     ]
    }
   ],
   "source": [
    "tweets_data_pol_test = data_pol_test['text'].values\n",
    "tweets_data_pol_test = tokenizer.texts_to_sequences(data_pol_test['text'].values)\n",
    "tweets_data_pol_test = pad_sequences(tweets_data_pol_test, maxlen=maxlen)\n",
    "labels_data_pol_test = pd.get_dummies(data_pol_test['sent']).values\n",
    "score,acc = model.evaluate(tweets_data_pol_test, labels_data_pol_test, verbose = 1, batch_size = batch_size)\n",
    "print(\"score: %.2f\" % (score))\n",
    "print(\"acc: %.2f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "0.0    1485\n",
      "1.0    1367\n",
      "Name: sent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "tweets_data_pol_train = data_pol_train['text'].values\n",
    "tweets_data_pol_train = tokenizer.texts_to_sequences(data_pol_train['text'].values)\n",
    "tweets_data_pol_train = pad_sequences(tweets_data_pol_train, maxlen=maxlen)\n",
    "labels_data_pol_train = pd.get_dummies(data_pol_train['sent']).values\n",
    "diff = data_pol['sent'].value_counts()[0] - data_pol['sent'].value_counts()[1]\n",
    "print (diff)\n",
    "    \n",
    "print(data_pol_train['sent'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "0s - loss: 0.1965 - acc: 0.9299\n",
      "Epoch 2/2\n",
      "0s - loss: 0.1714 - acc: 0.9355\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efcdc7d5940>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 512\n",
    "model.fit(tweets_data_pol_train, labels_data_pol_train, epochs = 2, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.220595 0.779405]\n",
      "100\n",
      "[0.9884454  0.01155459]\n",
      "223\n",
      "pos_acc 74.61928934010153 % 197\n",
      "neg_acc 85.94594594594595 % 185\n"
     ]
    }
   ],
   "source": [
    "pos_cnt, neg_cnt, pos_correct, neg_correct = 0, 0, 0, 0\n",
    "for x in range(len(tweets_data_pol_test)):\n",
    "    \n",
    "    result = model.predict(tweets_data_pol_test[x].reshape(1,tweets_test.shape[1]),batch_size=1,verbose = 2)[0]\n",
    "    \n",
    "    if random.uniform(0, 1) < 0.001:\n",
    "        print(result)\n",
    "        print(x)\n",
    "    \n",
    "    if result[0]>result[1]:\n",
    "        conf = result[0]\n",
    "    else:\n",
    "        conf = result[1]\n",
    "   \n",
    "    if conf > 0.80:\n",
    "        if np.argmax(result) == np.argmax(labels_data_pol_test[x]):\n",
    "            if np.argmax(labels_data_pol_test[x]) == 0:\n",
    "                neg_correct += 1\n",
    "            else:\n",
    "                pos_correct += 1\n",
    "\n",
    "        if np.argmax(labels_kaggle[x]) == 0:\n",
    "            neg_cnt += 1\n",
    "        else:\n",
    "            pos_cnt += 1\n",
    "\n",
    "\n",
    "\n",
    "print(\"pos_acc\", pos_correct/pos_cnt*100, \"%\", pos_cnt)\n",
    "print(\"neg_acc\", neg_correct/neg_cnt*100, \"%\", neg_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.80771905 0.19228093]]\n"
     ]
    }
   ],
   "source": [
    "text = 'Questa cosa non non non  mi piace particolarmente Renzi'\n",
    "text = [text]\n",
    "text = tokenizer.texts_to_sequences(text)\n",
    "text = pad_sequences(text, maxlen = 30)\n",
    "\n",
    "print(model.predict(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
